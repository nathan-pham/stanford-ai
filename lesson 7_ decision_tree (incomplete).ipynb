{"cells":[{"cell_type":"markdown","metadata":{"id":"r9-pi4P_pQF5"},"source":["# Decision Tree\n","\n","Decison trees are one of the most basic machine learning algorithms. In a classification problem, the training data are splitted using binary questions. The questions are asked such that most information is gained from each question.\n","When the algorithm is converged/terminated, we will have a number of leaf nodes with associated labels. During testing, the same questions are asked until a single leaf node is reached. The label of that node is chosen as the predicted label. \n","\n","## Gini Score\n","Below we write a function to compute Gini score on a data-label pair `(X, y)` where `X` is a 2d numpy array of data and `y` is the 1d numpy array of labels. \n","Notice that columns of `X` are features and rows of `X` are different data inputs. "]},{"cell_type":"code","execution_count":1,"metadata":{"id":"CP5NzWWNfeRl"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GUuQBKPqop-G"},"outputs":[],"source":["def lesson7_q1_compute_gini(X, y, split_idx, split_val):\n","    \"\"\"\n","    this function computes the gini score after splitting \n","    the data by the following condition: `X[:, split_idx] < split_val`. \n","    \n","    inputs\n","    :param X: 2d numpy array of data\n","    :param y: 1d numpy array of labels\n","    :param split_idx: index of the splitting feature (column of X), integer\n","    :param split_val: value of the splitting feature, floating number\n","\n","    outputs:\n","    value of Gini score after splitting. \n","    \"\"\"\n","    # array to keep the count of each class on left and right splits\n","    n_class = len(np.unique(y))\n","    count_left = np.zeros(n_class)\n","    count_right = np.zeros(n_class)\n","\n","    # update the count arrays\n","    for (row, label) in zip(X, y):\n","        if row[split_idx] < split_val:\n","            count_left[label] += 1\n","        else:\n","            count_right[label] += 1\n","    \n","    # compute gini score of left split\n","    gini_left = 0 if count_left.sum() == 0 else 1 - ((count_left / count_left.sum()) ** 2).sum() \n","    gini_right = 0 if count_right.sum() == 0 else 1 - ((count_right / count_right.sum()) ** 2).sum() \n","\n","    # find the overall score as the weight sum of the left and right scores\n","    weight = count_left.sum() + count_right.sum()\n","    gini_total = (count_left.sum() / weight) * gini_left + (count_right.sum() / weight) * gini_right\n","    return gini_total"]},{"cell_type":"markdown","metadata":{"id":"K1PACRfKqt-5"},"source":["# Finding the best split\n","\n","Now we have to iterate on all features, and all possible values to get the best split\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TS5gKvUsqtgY"},"outputs":[],"source":["def lesson7_q2_get_best_split(X, y, print_flag=False):\n","    \"\"\"\n","    interate over all features (columns of X) and all values (rows of X),\n","    split the data, compute the gini index, keep the pair with the lowest gini score\n","\n","    inputs:\n","    - X: input data, must be 2d numpy array\n","    - y: label data, must be 1d numpy array\n","    - print_flag: if True, the scores on all trials will be printed. default is False\n","    \n","    outputs:\n","    - best_split_dicta dictionary with keys [\"split_idx\", \"split_value\", \"split_score\"] and \n","              the correspoing values of the best split.  \n","    \"\"\"\n","\n","    nrow, ncol = X.shape\n","    best_split_idx = None\n","    best_split_value = None\n","    best_split_score = np.infty\n","\n","    for idx in range(ncol):  # iterate over index\n","        values = X[:, idx] # select the row idx of data . write your code here\n","        for value in values: # iterate over values\n","            score = lesson7_q1_compute_gini(X, y, idx, value) # compute the gini score\n","            if print_flag:\n","                print(\"idx=%d, value=%0.3f, score=%0.3f\" %(idx, value, score))\n","            if score < best_split_score: # update if score is improved.\n","                best_split_idx   = idx # write your code here\n","                best_split_value = value # write your code here\n","                best_split_score = score # write your code here\n","\n","            if score == 0:\n","              break\n","            # exit the loop, if a perfect split is already found.\n","            # write your code here (~2 lines)\n","\n","    return best_split_idx, best_split_value, best_split_score"]},{"cell_type":"markdown","metadata":{"id":"cwM6qtH2BpTy"},"source":["If you want to fully build a decision tree, you would need to progressively split the data using the method introduced above, and then construct a tree-like structure. This step is outside the scope of this course."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN2eRSGVyVlcZRJBFq+bMOp","collapsed_sections":[],"name":"lesson 7: decision_tree (incomplete).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"vscode":{"interpreter":{"hash":"0944a0c3b24f1c261af4dc0da5c56518cca690f2389022f5be6d63d3783b15a9"}}},"nbformat":4,"nbformat_minor":0}
